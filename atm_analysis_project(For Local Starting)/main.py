# main.py
import os
import warnings
from src import config, data_loader, geometry_utils, analysis, visualizer

# –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
warnings.filterwarnings('ignore')

def main():
    # 0. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞–ø–æ–∫
    if not os.path.exists('data'):
        os.makedirs('data')
    
    print("=== –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê –õ–û–ö–ê–¶–ò–ô –ë–ê–ù–ö–û–ú–ê–¢–û–í ===")

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    area_gdf, area_proj = data_loader.download_roi(config.PLACE_NAME)
    data_layers = data_loader.download_infrastructure(area_proj.geometry.iloc[0])
    
    # 2. –°–æ–∑–¥–∞–Ω–∏–µ —Å–µ—Ç–∫–∏ –∏ —Ñ–∏—á–µ–π
    grid_gdf = geometry_utils.generate_h3_grid(area_gdf)
    processed_df = geometry_utils.engineer_features(grid_gdf, data_layers)
    
    # 3. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏ –ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
    processed_df, X_scaled, feature_cols = analysis.run_clustering(processed_df)
    result_df, feat_imp = analysis.train_model(processed_df, X_scaled, feature_cols)
    
    if result_df is None:
        print("–û—Å—Ç–∞–Ω–æ–≤–∫–∞: –º–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞.")
        return

    # 4. –í—ã–±–æ—Ä —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π (—Ç–∞–º, –≥–¥–µ –Ω–µ—Ç –±–∞–Ω–∫–æ–º–∞—Ç–æ–≤)
    candidates = result_df[result_df['atm_target'] == 0].copy()
    top_20 = candidates.sort_values('potential_score', ascending=False).head(20)
    
    print(f"\nüèÜ –¢–æ–ø-20 –ª–æ–∫–∞—Ü–∏–π (–°—Ä–µ–¥–Ω–∏–π —Å–∫–æ—Ä: {top_20['potential_score'].mean():.3f})")
    
    # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    csv_path = "data/recommendations.csv"
    top_20.drop(columns=['geometry']).to_csv(csv_path)
    print(f"üíæ –¢–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {csv_path}")
    
    # 6. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
    visualizer.create_map(candidates, top_20, "data/map.html")
    
    print("\n=== –ì–û–¢–û–í–û ===")

if __name__ == "__main__":
    main()